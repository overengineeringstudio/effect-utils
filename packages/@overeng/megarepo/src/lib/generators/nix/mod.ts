/**
 * Nix Workspace Generator
 *
 * Generates a local Nix workspace under `.direnv/megarepo-nix/workspace`
 * and writes `.envrc.generated.megarepo` for direnv integration.
 */

import { Command, FileSystem } from '@effect/platform'
import { Effect, Schema } from 'effect'

import {
  EffectPath,
  getMemberPath,
  type AbsoluteDirPath,
  type AbsoluteFilePath,
  type MegarepoConfig,
} from '../../config.ts'
import { MR_VERSION } from '../../version.ts'

/** Error when generating the Nix workspace fails */
export class NixGeneratorError extends Schema.TaggedError<NixGeneratorError>()(
  'NixGeneratorError',
  {
    message: Schema.String,
    cause: Schema.optional(Schema.Defect),
  },
) {}

/** Options for the Nix workspace generator */
export interface NixGeneratorOptions {
  /** Path to the outermost megarepo root */
  readonly megarepoRootOutermost: AbsoluteDirPath
  /** Path to the nearest megarepo root */
  readonly megarepoRootNearest: AbsoluteDirPath
  /** The megarepo config */
  readonly config: typeof MegarepoConfig.Type
}

/** Result of the Nix workspace generation */
export interface NixGeneratorResult {
  readonly workspaceRoot: AbsoluteDirPath
  readonly flakePath: AbsoluteFilePath
  readonly envrcPath: AbsoluteFilePath
}

interface WorkspaceFlakeRepo {
  readonly name: string
  readonly path: AbsoluteDirPath
}

const defaultWorkspaceDir = '.direnv/megarepo-nix/workspace/'

const normalizeWorkspaceDir = (rawDir: string): string =>
  rawDir.endsWith('/') ? rawDir : `${rawDir}/`

const pad = (value: number): string => value.toString().padStart(2, '0')

const formatTimestamp = (date: Date = new Date()): string => {
  const offsetMinutes = -date.getTimezoneOffset()
  const sign = offsetMinutes >= 0 ? '+' : '-'
  const absOffset = Math.abs(offsetMinutes)
  const offsetHours = pad(Math.floor(absOffset / 60))
  const offsetMins = pad(absOffset % 60)

  return `${date.getFullYear()}-${pad(date.getMonth() + 1)}-${pad(date.getDate())} ${pad(
    date.getHours(),
  )}:${pad(date.getMinutes())}:${pad(date.getSeconds())} ${sign}${offsetHours}:${offsetMins}`
}

// Content hash prefix used in generated file headers for idempotent writes (R12)
const CONTENT_HASH_PREFIX = '# Content hash: '

/** Compute a simple hash of content for change detection */
const computeContentHash = (content: string): string => {
  // Simple djb2 hash - fast and sufficient for change detection
  let hash = 5381
  for (let i = 0; i < content.length; i++) {
    hash = (hash * 33) ^ content.charCodeAt(i)
  }
  // Convert to unsigned 32-bit and then to hex
  return (hash >>> 0).toString(16).padStart(8, '0')
}

/** Extract content hash from existing file header, if present */
export const extractContentHash = (fileContent: string): string | undefined => {
  const lines = fileContent.split('\n')
  for (const line of lines.slice(0, 10)) {
    // Only check first 10 lines (header area)
    if (line.startsWith(CONTENT_HASH_PREFIX)) {
      return line.slice(CONTENT_HASH_PREFIX.length).trim()
    }
  }
  return undefined
}

/** Build a header with metadata including content hash */
const buildHeader = ({
  contentHash,
  commentPrefix = '#',
}: {
  contentHash: string
  commentPrefix?: string
}): string =>
  [
    `${commentPrefix} Generated by megarepo - do not edit manually`,
    `${commentPrefix} Regenerate with: mr generate nix`,
    `${commentPrefix} Generated at: ${formatTimestamp()}`,
    `${commentPrefix} MR version: ${MR_VERSION}`,
    `${commentPrefix} Content hash: ${contentHash}`,
    '',
  ].join('\n')

/** Check if file needs to be written based on content hash comparison */
export const shouldWriteGeneratedFile = (
  existingContent: string | undefined,
  newContent: string,
): boolean => {
  if (existingContent === undefined) return true
  const existingHash = extractContentHash(existingContent)
  if (existingHash === undefined) return true // No hash found, need to write
  const newHash = computeContentHash(newContent)
  return existingHash !== newHash
}

/** Create full file content with header containing content hash */
export const createGeneratedFileContent = (content: string): string => {
  const contentHash = computeContentHash(content)
  return buildHeader({ contentHash }) + content
}

/** Generate the functional content for .envrc.generated.megarepo (without header) */
export const generateEnvrcContent = ({
  options,
  workspaceRoot: _workspaceRoot,
}: {
  options: NixGeneratorOptions
  workspaceRoot: AbsoluteDirPath
}): string => {
  const memberNames = Object.keys(options.config.members).join(',')

  return `find_megarepo_root_nearest() {
  local current="$1"
  local last=""
  while [ -n "$current" ] && [ "$current" != "/" ] && [ "$current" != "$last" ]; do
    if [ -f "$current/megarepo.json" ]; then
      echo "$current"
      return 0
    fi
    last="$current"
    current="$(dirname "$current")"
  done
  return 1
}

find_megarepo_root_outermost() {
  local current="$1"
  local last=""
  local candidate=""
  while [ -n "$current" ] && [ "$current" != "/" ] && [ "$current" != "$last" ]; do
    if [ -f "$current/megarepo.json" ]; then
      candidate="$current"
    fi
    last="$current"
    current="$(dirname "$current")"
  done
  if [ -n "$candidate" ]; then
    echo "$candidate"
    return 0
  fi
  return 1
}

MEGAREPO_ROOT_NEAREST="$(find_megarepo_root_nearest "$(pwd)")" || {
  echo "megarepo: megarepo.json not found in parent dirs of $(pwd)" >&2
  return 1 2>/dev/null || exit 1
}
MEGAREPO_ROOT_OUTERMOST="$(find_megarepo_root_outermost "$MEGAREPO_ROOT_NEAREST")" || {
  echo "megarepo: megarepo.json not found above $MEGAREPO_ROOT_NEAREST" >&2
  return 1 2>/dev/null || exit 1
}
MEGAREPO_ROOT_NEAREST="\${MEGAREPO_ROOT_NEAREST%/}/"
MEGAREPO_ROOT_OUTERMOST="\${MEGAREPO_ROOT_OUTERMOST%/}/"

export MEGAREPO_ROOT_OUTERMOST
export MEGAREPO_ROOT_NEAREST
export MEGAREPO_MEMBERS="${memberNames}"
export MEGAREPO_NIX_WORKSPACE="\${MEGAREPO_ROOT_NEAREST}.direnv/megarepo-nix/workspace"

# Compute devenv override args for flake inputs (R12: content-addressed for stability)
# Uses git+file: with ref=HEAD to use only committed state, avoiding spurious re-evals
# from both timestamp changes (path:) and uncommitted changes (git+file: without ref)
#
# NOTE: We use an array (MEGAREPO_DEVENV_ARGS_ARRAY) for proper handling of special chars.
# The string form (MEGAREPO_DEVENV_ARGS) is also provided for compatibility, but the
# ? in the URL may be stripped by direnv's nullglob setting. Use the array form:
#   use devenv "\${MEGAREPO_DEVENV_ARGS_ARRAY[@]}"
MEGAREPO_DEVENV_ARGS_ARRAY=()
_megarepo_devenv_args=""
for _member in effect-utils; do
  _repo_path="\${MEGAREPO_ROOT_NEAREST}repos/\$_member"
  # Resolve symlinks to get actual path
  [ -L "\$_repo_path" ] && _repo_path="\$(readlink -f "\$_repo_path")"
  # Check for .git dir or file (worktrees use .git file)
  if [ -e "\$_repo_path/.git" ]; then
    # Use ref=HEAD to only consider committed state (ignores dirty tree)
    # This ensures shell stability even when files are being edited
    MEGAREPO_DEVENV_ARGS_ARRAY+=(--override-input "\$_member" "git+file:\$_repo_path?ref=HEAD")
    _megarepo_devenv_args="\$_megarepo_devenv_args --override-input \$_member 'git+file:\$_repo_path?ref=HEAD'"
  fi
done
export MEGAREPO_DEVENV_ARGS="\$_megarepo_devenv_args"
unset _member _repo_path _megarepo_devenv_args

# R12 Monitoring: Track shell reload reasons
# Captures flake input fingerprints and detects what changed between reloads
_megarepo_r12_state_file="\${MEGAREPO_ROOT_NEAREST}.direnv/megarepo-r12.json"
_megarepo_r12_enabled="\${MEGAREPO_R12_MONITOR:-1}"

_megarepo_r12_capture_fingerprints() {
  # Capture fingerprints from devenv verbose output (runs quickly, ~50ms)
  # Output format: attr_path: <name>, fingerprint: <hash>
  local fingerprints
  fingerprints=\$(devenv info --verbose --no-tui \$MEGAREPO_DEVENV_ARGS 2>&1 | \\
    grep -E '^attr_path:' | \\
    sed 's/attr_path: \\([^,]*\\), fingerprint: \\(.*\\)/{"\\1":"\\2"}/' | \\
    tr '\\n' ',' | sed 's/,\$//' | sed 's/^/{/;s/\$/}/')
  echo "\$fingerprints"
}

_megarepo_r12_monitor() {
  [ "\$_megarepo_r12_enabled" = "0" ] && return 0
  
  local state_dir="\$(dirname "\$_megarepo_r12_state_file")"
  [ ! -d "\$state_dir" ] && mkdir -p "\$state_dir"
  
  local timestamp=\$(date -Iseconds)
  local reload_count=0
  local prev_fingerprints=""
  local cache_status=""
  
  # Read previous state if exists
  if [ -f "\$_megarepo_r12_state_file" ]; then
    reload_count=\$(jq -r '.reload_count // 0' "\$_megarepo_r12_state_file" 2>/dev/null || echo 0)
    prev_fingerprints=\$(jq -c '.fingerprints // {}' "\$_megarepo_r12_state_file" 2>/dev/null || echo '{}')
  fi
  
  reload_count=\$((reload_count + 1))
  
  # Capture current fingerprints and cache status from devenv
  local devenv_output
  devenv_output=\$(devenv info --verbose --no-tui \$MEGAREPO_DEVENV_ARGS 2>&1)
  
  # Extract fingerprints
  local fingerprints
  fingerprints=\$(echo "\$devenv_output" | \\
    grep -E '^attr_path:' | \\
    awk -F', ' '{
      gsub(/attr_path: /, "", \$1);
      gsub(/fingerprint: /, "", \$2);
      printf "\\"%s\\":\\"%s\\",", \$1, \$2
    }' | sed 's/,\$//' | sed 's/^/{/;s/\$/}/')
  [ -z "\$fingerprints" ] && fingerprints='{}'
  
  # Detect cache status
  if echo "\$devenv_output" | grep -q "Cache hit"; then
    cache_status="hit"
  elif echo "\$devenv_output" | grep -q "Cached eval invalidated"; then
    cache_status="miss"
  else
    cache_status="unknown"
  fi
  
  # Compare fingerprints and find changes
  local changes="[]"
  if [ -n "\$prev_fingerprints" ] && [ "\$prev_fingerprints" != "{}" ]; then
    changes=\$(jq -nc --argjson prev "\$prev_fingerprints" --argjson curr "\$fingerprints" '
      [(\$prev | keys[]) as \$k | 
        if \$curr[\$k] != \$prev[\$k] then
          {input: \$k, from: \$prev[\$k], to: (\$curr[\$k] // "removed")}
        else empty end] +
      [(\$curr | keys[]) as \$k |
        if \$prev[\$k] == null then
          {input: \$k, from: null, to: \$curr[\$k]}
        else empty end]
    ')
  fi
  
  # Write new state
  jq -nc \\
    --arg ts "\$timestamp" \\
    --argjson count "\$reload_count" \\
    --argjson fp "\$fingerprints" \\
    --arg cache "\$cache_status" \\
    --argjson changes "\$changes" \\
    '{
      last_reload: \$ts,
      reload_count: \$count,
      fingerprints: \$fp,
      cache_status: \$cache,
      last_changes: \$changes
    }' > "\$_megarepo_r12_state_file"
  
  # Report if there were unexpected changes (R12 violation candidates)
  local change_count=\$(echo "\$changes" | jq 'length')
  if [ "\$change_count" -gt 0 ] && [ "\$cache_status" = "miss" ]; then
    echo "\\033[33mmegarepo R12: Shell reload #\$reload_count - \$change_count input(s) changed:\\033[0m" >&2
    echo "\$changes" | jq -r '.[] | "  \\(.input): \\(.from // \"(new)\") -> \\(.to)"' >&2
  fi
}

# Export function to check R12 status
megarepo_r12_status() {
  if [ ! -f "\$_megarepo_r12_state_file" ]; then
    echo "No R12 monitoring data yet. Shell hasn't been reloaded."
    return 0
  fi
  
  echo "=== Megarepo R12 Shell Stability Status ==="
  jq -r '
    "Last reload: \\(.last_reload)",
    "Total reloads: \\(.reload_count)",
    "Last cache status: \\(.cache_status)",
    "",
    "Current fingerprints:",
    (.fingerprints | to_entries[] | "  \\(.key): \\(.value[0:12])..."),
    "",
    if (.last_changes | length) > 0 then
      "Last changes:",
      (.last_changes[] | "  \\(.input): \\(.from[0:8] // \"(new)\")... -> \\(.to[0:8])...")
    else
      "No changes detected in last reload (cache hit expected)"
    end
  ' "\$_megarepo_r12_state_file"
}

# Run R12 monitoring in background to avoid slowing down shell startup
# The fingerprint capture adds ~100-200ms, so we do it async
if [ "\$_megarepo_r12_enabled" = "1" ]; then
  (_megarepo_r12_monitor &) 2>/dev/null
fi
`
}

/** Generate the content for the workspace flake.nix */
export const generateWorkspaceFlakeContent = (repos: readonly WorkspaceFlakeRepo[]): string => {
  const inputs = repos.map(({ name }) => `    "${name}" = { url = "path:./${name}"; };`).join('\n')

  return [
    '{',
    '  inputs = {',
    inputs,
    '  };',
    '',
    '  outputs = inputs@{ self, ... }:',
    '    let',
    '      repos = builtins.removeAttrs inputs [ "self" ];',
    '      systems = [',
    '        "x86_64-linux"',
    '        "aarch64-linux"',
    '        "x86_64-darwin"',
    '        "aarch64-darwin"',
    '      ];',
    '      packagesFor = system:',
    '        builtins.mapAttrs (_: repo:',
    '          if repo ? packages && builtins.hasAttr system repo.packages then',
    '            repo.packages.${system}',
    '          else',
    '            { }',
    '        ) repos;',
    '      appsFor = system:',
    '        builtins.mapAttrs (_: repo:',
    '          if repo ? apps && builtins.hasAttr system repo.apps then',
    '            repo.apps.${system}',
    '          else',
    '            { }',
    '        ) repos;',
    '      bySystem = f:',
    '        builtins.listToAttrs (map (system: { name = system; value = f system; }) systems);',
    '    in',
    '    {',
    '      repos = repos;',
    '      packages = bySystem packagesFor;',
    '      apps = bySystem appsFor;',
    '    };',
    '}',
    '',
  ].join('\n')
}

const rsyncRepo = Effect.fn('megarepo/nix/rsyncRepo')((source: string, dest: string) =>
  Effect.gen(function* () {
    // Mirror repos into the workspace without copying heavy build outputs.
    const args = [
      '-a',
      '--copy-links',
      '--delete',
      '--prune-empty-dirs',
      // Always include bun.lock, even if a repo's .gitignore would exclude it.
      '--filter',
      '+ bun.lock',
      '--filter',
      '+ **/bun.lock',
      '--filter',
      ':- .gitignore',
      '--exclude',
      '.git',
      '--exclude',
      '.direnv',
      '--exclude',
      '.devenv',
      '--exclude',
      '.envrc',
      '--exclude',
      '.envrc.local',
      '--exclude',
      '.envrc.generated.megarepo',
      '--exclude',
      'result',
      '--exclude',
      'tmp',
      '--exclude',
      'node_modules',
      `${source}/`,
      `${dest}/`,
    ]

    const command = Command.make('rsync', ...args).pipe(
      Command.stdout('inherit'),
      Command.stderr('inherit'),
    )

    const exitCode = yield* Command.exitCode(command).pipe(
      Effect.mapError(
        (cause) =>
          new NixGeneratorError({
            message: `rsync failed for ${source}`,
            cause,
          }),
      ),
    )

    if (exitCode === 23) {
      yield* Effect.logWarning(`rsync warning (code 23) while syncing ${source}`)
      return
    }

    if (exitCode !== 0) {
      return yield* new NixGeneratorError({
        message: `rsync failed for ${source}`,
      })
    }
  }),
)

/** Generate the Nix workspace for a megarepo */
export const generateNix = Effect.fn('megarepo/generate/nix')((options: NixGeneratorOptions) =>
  Effect.gen(function* () {
    const fs = yield* FileSystem.FileSystem
    const repoRoot = options.megarepoRootNearest
    const repoNames = Object.keys(options.config.members).toSorted()

    // Note: We allow empty member lists - all members may be skipped via --skip

    const workspaceDir = options.config.generators?.nix?.workspaceDir ?? defaultWorkspaceDir
    const workspaceRoot = EffectPath.ops.join(
      repoRoot,
      EffectPath.unsafe.relativeDir(normalizeWorkspaceDir(workspaceDir)),
    )

    yield* fs.makeDirectory(workspaceRoot, { recursive: true })

    const flakePath = EffectPath.ops.join(
      workspaceRoot,
      EffectPath.unsafe.relativeFile('flake.nix'),
    )
    const envrcPath = EffectPath.ops.join(
      repoRoot,
      EffectPath.unsafe.relativeFile('.envrc.generated.megarepo'),
    )
    const repoInputs: WorkspaceFlakeRepo[] = []
    const mirrorRepos: WorkspaceFlakeRepo[] = []
    for (const repoName of repoNames) {
      const source = getMemberPath({ megarepoRoot: repoRoot, name: repoName })
      const exists = yield* fs.exists(source)
      if (!exists) {
        // Gracefully skip missing members - they may have been excluded via --skip
        yield* Effect.logWarning(`Skipping ${repoName} in nix generator (path does not exist).`)
        continue
      }
      mirrorRepos.push({ name: repoName, path: source })

      const repoFlakePath = EffectPath.ops.join(source, EffectPath.unsafe.relativeFile('flake.nix'))
      if (yield* fs.exists(repoFlakePath)) {
        repoInputs.push({ name: repoName, path: source })
      } else {
        yield* Effect.logWarning(`Skipping ${repoName} in workspace flake (no flake.nix found).`)
      }
    }

    const flakeContent = generateWorkspaceFlakeContent(repoInputs)
    const envrcFunctionalContent = generateEnvrcContent({ options, workspaceRoot })

    // Write files only if content changed (avoids unnecessary direnv reloads)
    const flakeExists = yield* fs.exists(flakePath)
    if (!flakeExists || (yield* fs.readFileString(flakePath)) !== flakeContent) {
      yield* fs.writeFileString(flakePath, flakeContent)
    }

    // For envrc, use content hash to detect changes (R12: shell stability)
    // Only writes if functional content changed, ignoring metadata like timestamps
    const envrcExists = yield* fs.exists(envrcPath)
    const existingEnvrc = envrcExists ? yield* fs.readFileString(envrcPath) : undefined
    if (shouldWriteGeneratedFile(existingEnvrc, envrcFunctionalContent)) {
      yield* fs.writeFileString(envrcPath, createGeneratedFileContent(envrcFunctionalContent))
    }

    // Neutral .envrc content for workspace mirrors - prevents accidental direnv activation
    // and stops recursive devenv evaluation when the parent megarepo uses --override-input
    const neutralEnvrcContent = `# Workspace mirror - managed by megarepo
# This directory is a read-only mirror for Nix flake input resolution.
# Do not develop here; use the original repo or the repos/ symlinks instead.
#
# This .envrc intentionally does nothing to prevent recursive devenv evaluation.
`

    for (const repo of mirrorRepos) {
      const dest = EffectPath.ops.join(
        workspaceRoot,
        EffectPath.unsafe.relativeDir(`${repo.name}/`),
      )
      yield* fs.makeDirectory(dest, { recursive: true })
      yield* rsyncRepo(repo.path, dest)

      // Write neutral .envrc to prevent direnv from activating in workspace mirrors
      const mirrorEnvrcPath = EffectPath.ops.join(dest, EffectPath.unsafe.relativeFile('.envrc'))
      yield* fs.writeFileString(mirrorEnvrcPath, neutralEnvrcContent)
    }

    return { workspaceRoot, flakePath, envrcPath }
  }),
)
